{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text processing, data sources and corpora\n",
    "\n",
    "## Short intro to NLTK\n",
    "\n",
    "NLTK library organization ([modules](http://www.nltk.org/py-modindex.html)):\n",
    "\n",
    "| Module |\tShortcuts |\tData Structures\t| Interfaces|\tNLP Pyramid|\n",
    "|---|---|---|---|---|\n",
    "|*nltk.stem*, *nltk.text*, *nltk.tokenize* | *word_tokenize*, *sent_tokenize* |\t*str*, *nltk.Text* => *[str]* |\t*StemmerI*, *TokenizerI* |\t*Morphology* |\n",
    "|*nltk.tag*, *nltk.chunk*|\t*pos_tag*|\t*[str]* => *[(str, tag)]*, *nltk.Tree*|\t*TaggerI*, *ParserI*, *ChunkParserI*\t|Syntax|\n",
    "|*nltk.chunk*, *nltk.sem*|\t*ne_chunk*|\t*nltk.Tree*, *nltk.DependencyGraph*|\t*ParserI*, *ChunkParserI*|\tSemantics|\n",
    "|*nltk.sem.drt*\t|–|\t*Expression*\t|–|\tPragmatics|\n",
    "\n",
    "An example passing the first three levels of the NLP Pyramid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "text = \"John works at OBI.\" \n",
    " \n",
    "# Morphology Level\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    " \n",
    "# Syntax Level\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"POS tagging:\", tagged_tokens)\n",
    " \n",
    "# Semantics Level\n",
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "print(\"Light parsing:\", ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with text data, a [Text object](http://www.nltk.org/api/nltk.html#nltk.text.Text) might be useful to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "from nltk.corpus import reuters\n",
    " \n",
    "text = Text(reuters.words())\n",
    " \n",
    "print(\"Similar words to 'Monday':\")\n",
    "text.similar('Monday', 5)\n",
    "\n",
    "print(\"\\nCommon contexts to a list of words ('August', 'June'):\")\n",
    "text.common_contexts(['August', 'June'])\n",
    "\n",
    "print(\"\\nContexts of a word 'Monday':\")\n",
    "text.concordance('Monday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with n-grams (bigrams, trigrams) and collocations extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    " \n",
    "# Bigrams\n",
    "finder = BigramCollocationFinder.from_words(nltk.corpus.reuters.words())\n",
    "finder.apply_freq_filter(5)\n",
    "\n",
    "print(\"\\nBest 50 bigrams according to PMI:\", finder.nbest(bigram_measures.pmi, 50))\n",
    " \n",
    "# Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(nltk.corpus.reuters.words())\n",
    "finder.apply_freq_filter(5)\n",
    " \n",
    "print(\"\\nBest 50 trigrams according to PMI:\", finder.nbest(trigram_measures.pmi, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion between different data formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag import untag, str2tuple, tuple2str\n",
    "from nltk.chunk import tree2conllstr, conllstr2tree, conlltags2tree, tree2conlltags\n",
    " \n",
    "text = \"John works at OBI.\"\n",
    " \n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens: \", tokens)\n",
    " \n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"\\nTagged tokens: \", tagged_tokens)\n",
    " \n",
    "print(\"\\nUntagged tokens\", untag(tagged_tokens))\n",
    " \n",
    "tagged_tokens = [tuple2str(t) for t in tagged_tokens] \n",
    "print(\"\\nTagged tokens to strings:\", tagged_tokens)\n",
    " \n",
    "tagged_tokens = [str2tuple(t) for t in tagged_tokens]\n",
    "print(\"\\nTagged tokens from strings to tuples:\",  tagged_tokens)\n",
    " \n",
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "print(\"\\nNER tree:\", ner_tree)\n",
    " \n",
    "iob_tagged = tree2conlltags(ner_tree)\n",
    "print(\"\\nIOB tagged tree:\", iob_tagged)\n",
    " \n",
    "ner_tree = conlltags2tree(iob_tagged)\n",
    "print(\"\\nBack to tree:\", ner_tree)\n",
    " \n",
    "tree_str = tree2conllstr(ner_tree)\n",
    "print(\"\\nTree as CoNLL string:\\n\", tree_str)\n",
    " \n",
    "ner_tree = conllstr2tree(tree_str, chunk_types=('PERSON', 'ORGANIZATION'))\n",
    "print(\"\\nCoNLL string to tree:\", ner_tree)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting\n",
    "\n",
    "Generally, pretrained models are good enough for tokenization. You might encounter issues using them if you are working with a specific genre of text (e.g. technical with specific abbreviations) or working with an unsupported language.\n",
    "\n",
    "NLTK by default uses *PunktSentenceTokenizer*, which is unsupervised trainable model. A simple scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    " \n",
    "sentence = \"All FRI students could get a Msc. in Computer Science.\"\n",
    "print(\"Sentences:\", sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above obviously does not work correctly for us, let's train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.corpus import gutenberg\n",
    " \n",
    "# 1. Prepare text data\n",
    "text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    text += gutenberg.raw(file_id)\n",
    "\n",
    "# 2. Train the params\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    " \n",
    "# 3. Instantiate the model\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"Mr. James told me Dr. Zitnik is not available today. I will go to his office hours.\"\n",
    " \n",
    "print(\"Tokenized: \", tokenizer.tokenize(sentences))\n",
    "print(\"\\nLearned abbreviations:\", tokenizer._params.abbrev_types)\n",
    " \n",
    "from pprint import pprint\n",
    "print(\"\\nSplit decisions debugging:\")\n",
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print('=' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually add abbreviations or other parameters to update the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._params.abbrev_types.add('msc')\n",
    "tokenizer._params.abbrev_types.add('dr')\n",
    "\n",
    "sentence = \"All FRI students could get a Msc. in Computer Science.\"\n",
    "print(\"Tokenized:\", tokenizer.tokenize(sentence))\n",
    "\n",
    "sentences = \"Mr. James told me Dr. Zitnik is not available today. I will go to his office hours.\"\n",
    "print(\"Tokenized: \", tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text processing tools\n",
    "\n",
    "### Stemming\n",
    "\n",
    "> In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.\n",
    "\n",
    "One of the first stemmers was designed by [Martin Porter](https://tartarus.org/martin/PorterStemmer/) (*M.F. Porter, 1980, An algorithm for suffix stripping, Program, 14(3), pp. 130−137*), which is a suffix stripping algorithm and most widely used. Later he continued his work and presented his improved work for English and for some other languages with the Snowball stemmer.\n",
    "\n",
    "The NLTK includes some stemmer implementations - Porter, Lancaster, Snowball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Prepare text\n",
    "def getTokens():\n",
    "   with open('shakespeare.txt', 'r') as shakes:\n",
    "    text = shakes.read().lower()\n",
    "    # remove punctuation\n",
    "    table = text.maketrans({key: None for key in string.punctuation})\n",
    "    text = text.translate(table)      \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Do stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stems = [(token, stemmer.stem(token)) for token in getTokens()]\n",
    "stems[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "\n",
    "> Lemmatisation (or lemmatization) in linguistics is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "\n",
    "> In computational linguistics, lemmatisation is the algorithmic process of determining the lemma for a given word. Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence (requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.\n",
    "\n",
    "> In many languages, words appear in several inflected forms. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word.\n",
    "\n",
    "> Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in getTokens()]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the differences between lemmas and stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above finds a lemma expecting a noun by default, so we should also include a part-of-speech information, e.g., `lemmatizer.lemmatize(\"walking\", pos='v')` for verbs - the default parameter is noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to lemmatise is, are, was, were without and with pos parameter. Is there a difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging\n",
    "\n",
    "> A part of speech (abbreviated form: PoS or POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties. Words that are assigned to the same part of speech generally display similar behavior in terms of syntax—they play similar roles within the grammatical structure of sentences—and sometimes in terms of morphology, in that they undergo inflection for similar properties. Commonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner.\n",
    "\n",
    "In NLTK, there exist some implementations of taggers and possible tagsets ([source](http://www.nltk.org/_modules/nltk/tag.html#pos_tag))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posTagged = nltk.pos_tag(getTokens())\n",
    "posTagged[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the Penn Treebank tags are used ([tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"NNP\") # Meaning of a specific tag with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents comparison using TF-IDF\n",
    "\n",
    "TF-IDF is a common metric in information retrieval which weights a term with respect to a list of documents.\n",
    "\n",
    "> In information retrieval, tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "> Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
    "\n",
    "The metric is calculated as follows:\n",
    "\n",
    "$$\\textrm{TF}(term) = \\frac{\\textrm{Number of times term appears in a document}}{\\textrm{Total number of terms in the document}}$$\n",
    "\n",
    "$$\\textrm{IDF}(term) = \\frac{\\textrm{Total number of documents}}{\\textrm{Number of documents with the term in it}}$$\n",
    "\n",
    "$$\\textrm{TF-IDF}(term) = \\textrm{TF}(term) * \\log_e(\\textrm{IDF}(term))$$\n",
    "\n",
    "Computing TF-IDF scores for given documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer() # parameters for tokenization, stopwords can be passed\n",
    "tfidf = vect.fit_transform([\"Halloween is scary!\",\n",
    "                            \"People wear halloween masks.\",\n",
    "                            \"There exist many masks and halloween pumpkins.\",\n",
    "                            \"People build scary lantern pumpkins.\"])\n",
    "\n",
    "print(\"TF-IDF vectors (each column is a document):\\n{}\\nRows:\\n{}\".format(tfidf.T.A, vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing [cosine similarities](https://en.wikipedia.org/wiki/Cosine_similarity) for documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = (tfidf * tfidf.T).A\n",
    "print(\"Cosine similarity between the documents: \\n{}\".format(cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing a new document (or searching) to the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = vect.transform([\"Where to buy a halloween mask and pumpkins?\"])\n",
    "\n",
    "# HINT: If the text is completely different from the corpus, a zero vector will be returned\n",
    "# and therefore also not printed.\n",
    "print(\"New document:\\n{}\".format(weights.T.A)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to calculate, which documents are the most similar to a query above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful data sources/tools\n",
    "\n",
    "When recognizing important parts of text, it is useful to help yourself with semantic databases or gazetteer lists (e.g., lists of organization names, countries, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "> WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. WordNet can thus be seen as a combination of dictionary and thesaurus. While it is accessible to human users via a web browser, its primary use is in automatic text analysis and artificial intelligence applications. The database and software tools have been released under a BSD style license and are freely available for download from the WordNet website. Both the lexicographic data (lexicographer files) and the compiler (called grind) for producing the distributed database are available.\n",
    "\n",
    "Let's find all sets of synonyms for word `book`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"book\")\n",
    "syns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some data about these synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in syns:\n",
    "    print(\"Syset name: '{}'\".format(synset.name()))\n",
    "    print(\"Lemmas:     '{}'\".format([lemma.name() for lemma in synset.lemmas()]))\n",
    "    print(\"Definition: '{}'\".format(synset.definition()))\n",
    "    print(\"Examples:   '{}'\\n\".format(synset.examples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find synonyms, antonyms, hypernyms and hyponyms of a word `good`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "hypernyms = []\n",
    "hyponyms = []\n",
    "\n",
    "for synset in wordnet.synsets(\"good\"):\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "        if synset.hypernyms():\n",
    "            hypernyms.append(synset.hypernyms()[0].name())\n",
    "        if synset.hyponyms():\n",
    "            hyponyms.append(synset.hyponyms()[0].name())\n",
    "\n",
    "print(\"Synonyms:  '{}'\\n\".format(set(synonyms)))\n",
    "print(\"Antonyms:  '{}'\".format(set(antonyms)))\n",
    "print(\"Hypernyms: '{}'\".format(set(hypernyms)))\n",
    "print(\"Hyponyms:  '{}'\".format(set(hyponyms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare similarity of two synsets.\n",
    "\n",
    "The Wu & Palmer measure calculates relatedness by considering the depths of the two synsets in the WordNet taxonomies, along with the depth of the LCS. The formula is score = 2*depth(lcs) / (depth(s1) + depth(s2)). This means that 0 < score <= 1. The score can never be zero because the depth of the LCS is never zero (the depth of the root of a taxonomy is one). The score is one if the two input synsets are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = wordnet.synsets(\"good\")[0]\n",
    "nice = wordnet.synsets(\"nice\")[0]\n",
    "bad  = wordnet.synsets(\"bad\")[0]\n",
    "\n",
    "print(\"Good vs. nice: {:.2}\".format(good.wup_similarity(nice)))\n",
    "print(\" Good vs. bad: {:.2}\".format(good.wup_similarity(bad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other sources\n",
    "\n",
    "A plethora of other data sources exist, for example:\n",
    "\n",
    "* [VerbNet](http://verbs.colorado.edu/~mpalmer/projects/verbnet.html)\n",
    "* [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/)\n",
    "* [SentiStrength](http://sentistrength.wlv.ac.uk/)\n",
    "* [BabelNet](http://babelnet.org/)\n",
    "* [ConceptNet](http://www.conceptnet.io/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora\n",
    "\n",
    "To train or test an NLP system, we need some tagged data to compare the systems to. Some datasets exist from challenges, for example:\n",
    "\n",
    "* [CoNLL](http://www.conll.org/previous-tasks)\n",
    "* [SemEval](https://en.wikipedia.org/wiki/SemEval)\n",
    "* BioNLP [2016](http://2016.bionlp-st.org/), [2013](http://2013.bionlp-st.org/), [2011](http://2011.bionlp-st.org/), [2009](http://www.nactem.ac.uk/tsujii/GENIA/SharedTask/)\n",
    "* [CHEMDNER](http://www.biocreative.org/tasks/biocreative-iv/chemdner/)\n",
    "* [ACE2004](https://catalog.ldc.upenn.edu/LDC2005T09)\n",
    "* [Enron email dataset](https://www.cs.cmu.edu/~./enron/)\n",
    "* [Citeseer](http://csxstatic.ist.psu.edu/about/data)\n",
    "* [Reuters](http://www.daviddlewis.com/resources/testcollections/reuters21578/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one of the NLP datasets or extract your own and then:\n",
    "\n",
    "* Overview its structure.\n",
    "* Report on dataset features (number of tags, words, sentences, ...)\n",
    "* Use some existing data source and define some manual rules to tackle the dataset problem. Report on the accuracy of your method (percentage of correct labels) and compare to baseline or existing work.\n",
    "\n",
    "You can also visualize data using [Matplotlib](http://matplotlib.org/gallery.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
