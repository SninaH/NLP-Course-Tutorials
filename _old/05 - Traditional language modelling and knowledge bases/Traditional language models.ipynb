{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional language models\n",
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [slavko.zitnik@fri.uni-lj.si](mailto:slavko.zitnik@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "A model (from statistical point of view) is a mathematical representation of a process. Models may be an approximation of a process and there are two important reasons for this: \n",
    "\n",
    "1. We observe the process a limited amount of times.\n",
    "2. A model can be very complex so we should normally simplify it.\n",
    "\n",
    "In statistics we may have heard: `All models are wrong, but some are useful.`\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "We have already seen some models. One of them and also the simplest one is bag-of-words model, which is a naive way of modelling human language. But still, it is useful and popular. For the bag-of-words model we also know: \n",
    "\n",
    "1. It has an oversimplified view of the language.\n",
    "2. It takes into account only the frequency of the words in the language, not their order or position.\n",
    "3. In a way we have created it, it was useful for tasks such as text classification or sentiment analysis, where we were interested only into separate words and their count.\n",
    "\n",
    "## n-Grams\n",
    "\n",
    "Text is always a sequence - a sequence of words, characters, symbols, ... So one idea might be to model how text is generated or which token is most probably to proceed in a given sequence. We can learn probabilities over two tokens (bigrams), three tokens (trigrams), ... n tokens (n-grams).\n",
    "\n",
    "\"Bigram\" is just a fancy name for 2 consecutive words while n-gram is an n-tuple of consecutive tokens. Let's show a quick example of using word-based n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: \n",
      "\t['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "\n",
      "\n",
      "Bigrams: \n",
      "\t[('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n",
      "\n",
      "\n",
      "Padded bigrams: \n",
      "\t[(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n",
      "\n",
      "\n",
      "Trigrams: \n",
      "\t[('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n",
      "\n",
      "\n",
      "Padded trigrams: \n",
      "\t[(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "import operator\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    " \n",
    "first_sentence = reuters.sents()[0]\n",
    "print(\"First sentence: \\n\\t{}\\n\\n\".format(first_sentence))\n",
    " \n",
    "print(\"Bigrams: \\n\\t{}\\n\\n\".format(list(bigrams(first_sentence))))\n",
    "print(\"Padded bigrams: \\n\\t{}\\n\\n\".format(list(bigrams(first_sentence, pad_left=True, pad_right=True))))\n",
    "print(\"Trigrams: \\n\\t{}\\n\\n\".format(list(trigrams(first_sentence))))\n",
    "print(\"Padded trigrams: \\n\\t{}\\n\\n\".format(list(trigrams(first_sentence, pad_left=True, pad_right=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'What the economists' trigram occurence number: 2\n",
      "'Hell' follows 'What the' in 0 cases.\n",
      "8839 sentences start with 'The'\n",
      "\n",
      "\n",
      "'What the economists' trigram probability in given text: 0.0435\n",
      "The probability of 'Hell' following 'What the' is 0.0.\n",
      "The probability of a sentence to start with 'The' is 0.162.\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the data and count occurences\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "print(\"'What the economists' trigram occurence number: {}\".format(model[\"what\", \"the\"][\"economists\"]))\n",
    "print(\"'Hell' follows 'What the' in {} cases.\".format(model[\"what\", \"the\"][\"hell\"]))\n",
    "print(\"{} sentences start with 'The'\\n\\n\".format(model[None, None][\"The\"])) \n",
    "\n",
    "# 2. transform occurences to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "print(\"'What the economists' trigram probability in given text: {:.3}\".format(model[\"what\", \"the\"][\"economists\"]))\n",
    "print(\"The probability of 'Hell' following 'What the' is {:.3}.\".format(model[\"what\", \"the\"][\"hell\"]))\n",
    "print(\"The probability of a sentence to start with 'The' is {:.3}.\".format(model[None, None][\"The\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "#### Greedy approach\n",
    "Algorithm: Select the most probable word given last n-1 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 0.0034298705414246475\n",
      "\n",
      "Token probabilities: \n",
      "\t'0.16154324146501936 0.13055775540219483 0.6303797468354431 0.2580115036976171 0.9998732251521298 1.0'\n",
      "Generated sequence: \n",
      "\t'The company said .'\n"
     ]
    }
   ],
   "source": [
    "# 3. Use the model (e.g. text generation)\n",
    "text = [None, None]\n",
    "sentence_finished = False\n",
    "probs = []\n",
    "\n",
    "while not sentence_finished:\n",
    "    token = max(model[tuple(text[-2:])].items(), key=operator.itemgetter(1))\n",
    "    text.append(token[0])\n",
    "    probs.append(token[1])\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        break\n",
    "\n",
    "print(f\"Probability of text: {reduce(operator.mul, probs, 1)}\\n\")\n",
    "print(f\"Token probabilities: \\n\\t'{' '.join([str(prob) for prob in probs if token])}'\")\n",
    "print(f\"Generated sequence: \\n\\t'{' '.join([token for token in text if token])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the example above **greedy decoding** which will always yied the same result given input. To generate more useful text, tokens should be selected more random and taking into account cumulative scores. For example, there are some options:\n",
    "\n",
    "* **Beam search** is also a deterministic decoding and offers an improvement over greedy decoding. A problem of greedy decoding is that we might miss the most likely sequence since we predict only the most probable word at each timestep. Beam search mitigates this by keeping a track of most probable n sequences at every step and ultimately selecting the most probable sequence.\n",
    "* **Top *k* sampling** selects k most probable words and distributes their comulative probability over them. The problem is that we must choose a fixed sized parameter k which might lead to suboptimal results in some scenarios.\n",
    "* **Top *p* sampling** addresses this by selecting top words whose cumulative probability just exceeds p. This comulative probability is then again distributed among these words.\n",
    "\n",
    "A randomized example below, which shows there are no rules and you might use your imagination in NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the model (e.g. text generation)\n",
    " \n",
    "def rand_ngram_generator(initial):\n",
    "    text = initial\n",
    "    sentence_finished = False\n",
    "    prob = 1.0\n",
    "\n",
    "    while not sentence_finished:\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        for word in model[tuple(text[-2:])].keys():\n",
    "            accumulator += model[tuple(text[-2:])][word] \n",
    "\n",
    "            if accumulator >= r:\n",
    "                prob *= model[tuple(text[-2:])][word]\n",
    "                text.append(word)\n",
    "                break\n",
    "\n",
    "        if text[-2:] == [None, None]:\n",
    "            sentence_finished = True\n",
    "\n",
    "    print(f\"Probability of text: {prob}\\n\")\n",
    "    print(f\"Generated sequence: \\n\\t'{' '.join([token for token in text if token])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 1.6389479350819981e-56\n",
      "\n",
      "Generated sequence: \n",
      "\t'But he said in its mid - Mississippi River ( ex Chicago ) 100 pct of the year , not the central bank intervention at 143 . 75 dlrs Net profit 686 , 000 Revs 4 , 142 , 095 , 991 vs profit 123 , 500 in the previous week , Schlesinger told Reuters that European governments unfairly subsidise the Airbus project .'\n"
     ]
    }
   ],
   "source": [
    "rand_ngram_generator([None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 3.5612535612535607e-06\n",
      "\n",
      "Generated sequence: \n",
      "\t'I am sure that something they might have for acquisitions .'\n"
     ]
    }
   ],
   "source": [
    "rand_ngram_generator([\"I\", \"am\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 6.171160366632004e-10\n",
      "\n",
      "Generated sequence: \n",
      "\t'They were rather a reduction in output from the December 1986 .'\n"
     ]
    }
   ],
   "source": [
    "rand_ngram_generator([\"They\", \"were\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Collect as many as possible Christmas and New year wishes (in Slovene). Then analyse your corpus and train a simple language model that would generate a wish for your close ones.\n",
    "\n",
    "Implement beam search, different sampling techniques and compare results. Use some other and larger corpora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-course-fri] *",
   "language": "python",
   "name": "conda-env-nlp-course-fri-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
