{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text or Document Clustering\n",
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [slavko.zitnik@fri.uni-lj.si](mailto:slavko.zitnik@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "> Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering.\n",
    "\n",
    "Clustering vs. Categorization:\n",
    "* Categorization (supervised machine learning): To group objects into predetermined categories.\n",
    "  Needs a representation of the objects, a similarity measure and a training set.\n",
    "* Clustering (unsupervised machine learning): To divide a set of objects into clusters (parts of the set) so that objects in the same cluster are similar to each other, and/or objects in different clusters are dissimilar. Needs a representation of the objects and a similarity measure.\n",
    "\n",
    "Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.\n",
    "\n",
    "In general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method. By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the K-means algorithm and its variants. Generally hierarchical algorithms produce more in-depth information for detailed analyses, while algorithms based around variants of the K-means algorithm are more efficient and provide sufficient information for most purposes.\n",
    "\n",
    "The two types of algorithms:\n",
    "* Partitioning algorithms\n",
    "    1. Initial partition, for example: pick k documents at random as first cluster centroids.\n",
    "    2. Put each document in the most similar cluster.\n",
    "    3. Calculate new cluster centroids.\n",
    "    4. Repeat 2 and 3 until some condition is fulfilled.\n",
    "* Hierarchical algorithms\n",
    "    1. Make one cluster for each document.\n",
    "    2. Join the most similar pair into one cluster. \n",
    "    3. Repeat 2 until some condition is fulfilled.\n",
    "\n",
    "***\n",
    "\n",
    "There exist many tools to perform document clustering. One of them is [Ontogen](http://ontogen.ijs.si/), which automatically groups similar documents and visualizes them.\n",
    "\n",
    "Below we will try to perform document clustering using Python. For the task we will use IMDB data - genres, titles and synopses for 100 movies. Example used by Brandon Rose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Import data\n",
    "movies = pd.read_csv(\"movies_data.txt\", sep='\\t')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = movies[\"Genres\"].tolist()\n",
    "titles = movies[\"Title\"].tolist()\n",
    "synopses = movies[\"Synopses\"].tolist()\n",
    "\n",
    "print(\"First 10 movies in a dataset '{}.'\".format(movies[\"Title\"].tolist()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Below we define two functions that:\n",
    "\n",
    "* tokenize and also stem each token\n",
    "* tokenize each token only\n",
    "\n",
    "We use both these functions to create a dictionary as we will use stems for the clustering algorithm. We will need to convert stems back to their full words for the better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SnowballStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # First tokenize by sentence, then by word to ensure that punctuation is caught as it's own token.\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # Filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation).\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # First tokenize by sentence, then by word to ensure that punctuation is caught as it's own token.\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # Filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation).\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the preprocessing functions to iterate over the list of synopses to create two vocabularies: one stemmed and one only tokenized. \n",
    "\n",
    "Using these two, we create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. The benefit of this is it provides an efficient way to look up a stem and return a full token. The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in synopses:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "\n",
    "print(\"There are '{}' items in our data frame.\".format(str(vocab_frame.shape[0])))\n",
    "print(\"Data frame contents: \\n{}\".format(vocab_frame.head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and document similarity\n",
    "\n",
    "Now we define a term frequency-inverse document frequency (TF-IDF) vectorizer parameters and then convert the synopses list into a TF-IDF matrix.\n",
    "\n",
    "![tfidf.png](tfidf.png)\n",
    "\n",
    "The parameters description:\n",
    "\n",
    "* max_df: The maximum frequency within the documents a given feature can have to be used in the TF-IDF matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses).\n",
    "* min_idf: This could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here we pass 0.2; the term must be in at least 20% of the document.\n",
    "* ngram_range: This means we will look at unigrams, bigrams and trigrams. [More about ngrams](https://en.wikipedia.org/wiki/N-gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "                        max_df=0.8, \n",
    "                        max_features=200000,\n",
    "                        min_df=0.2, \n",
    "                        stop_words='english', \n",
    "                        use_idf=True, \n",
    "                        tokenizer=tokenize_and_stem, \n",
    "                        ngram_range=(1,3))\n",
    "\n",
    "# Fit the vectorizer to synopses texts\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) \n",
    "\n",
    "print(\"TF-IDF matrix shape: {}\".format(tfidf_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract terms for the features and calculate distances between documents.\n",
    "\n",
    "Terms is just a list of the features used in the TF-IDF matrix (vocabulary).\n",
    "\n",
    "Dist is defined as 1 - the cosine similarity of each document. Cosine similarity is measured against the TF-IDF matrix and can be used to generate a measure of similarity between each document and the other documents in the corpus (each synopsis among the synopses). Subtracting it from 1 provides cosine distance which I will use for plotting on a euclidean (2-dimensional) plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "Now the fun part begins ... \n",
    "\n",
    "K-means initializes with a pre-determined number of clusters (we choose 5). Each observation is assigned to a cluster (cluster assignment) so as to minimize the within cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# Perform clustering\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "print(\"Clusters: {}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use joblib.dump to pickle the model (save it and load it later from disk), once it has converged and to reload the model/reassign the labels as the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Uncomment the below line to save your model \n",
    "#joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "# Uncomment the below line to load your saved model \n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "#clusters = km.labels_.tolist()\n",
    "print(\"Clusters: {}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dictionary of titles, the synopsis, the cluster assignment, and the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = { \"title\": titles, \"synopsis\": synopses, \"cluster\": clusters, \"genre\": genres }\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = [\"title\", \"cluster\", \"genre\"])\n",
    "\n",
    "print(\"Number of movies per cluster: \\n{}\".format(frame[\"cluster\"].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform indexing and sorting on each cluster to identify which are the top n (n=6) words that are nearest to the cluster centroid. This gives a good sense of the main topic of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\\n\")\n",
    "\n",
    "# Sort cluster centers by proximity to centroid.\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "# Helper function\n",
    "def getClusterWords(cluster, n=6):\n",
    "    words = []\n",
    "    for ind in order_centroids[cluster, :n]: # Print 6 words per cluster\n",
    "        # old pandas version\n",
    "        #words.append(vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0])\n",
    "        words.append(vocab_frame.loc[terms[ind].split(' '),].values.tolist()[0][0])\n",
    "    return \", \".join(words)\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster {} words: {}\".format(i, getClusterWords(i)))\n",
    "    \n",
    "    print(\"Cluster {} titles:\".format(i), end='')\n",
    "    # old pandas version\n",
    "    #for title in frame.ix[i]['title'].values.tolist():\n",
    "\n",
    "    for title in frame[frame[\"cluster\"]==i][\"title\"].values.tolist():\n",
    "        print(\" {},\".format(title), end='')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the clusters\n",
    "\n",
    "First we need to convert the dist matrix into a 2-dimensional array using multidimensional scaling. We use an MDS algorithm. Another option would be to use principal component analysis method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Parameter \"precomputed\" because we provide a distance matrix.\n",
    "# Parameter \"random_state\" so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "# Shape of the result will be (n_components, n_samples).\n",
    "pos = mds.fit_transform(dist)  \n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize the document clustering output using matplotlib and [mpld3](https://mpld3.github.io/) (a matplotlib wrapper for [D3.js](http://d3js.org/) library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for clusters.\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "# Define cluster names\n",
    "cluster_names = dict([(i, getClusterWords(i, 3)) for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# Enable to draw plot inline.\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a data frame that has the result of the MDS plus the cluster numbers and titles.\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "# Group by cluster.\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# Set up plot.\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "# Iterate through groups to layer the plot.\n",
    "# Note that we use the cluster_name and cluster_color dicts with the 'name' \n",
    "# lookup to return the appropriate color/label.\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',         # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',        # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "# Add label in x,y position with the label as the film title.\n",
    "for i in range(len(df)):\n",
    "    # old pandas:\n",
    "    #ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8) \n",
    "    ax.text(df.loc[df.index[i], 'x'], df.loc[df.index[i], 'y'], df.loc[df.index[i], 'title'], size=8)  \n",
    "\n",
    "# Uncomment the below to show or save the plot.\n",
    "plt.show()                                       #show the plot\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200) # save the plot as an image \n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Animated JS-based visualization (do not worry about the implementation, for more information, check mpld3 documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpld3\n",
    "\n",
    "# Define custom toolbar location.\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}\n",
    "        \n",
    "# Create a data frame that has the result of the MDS plus the cluster numbers and titles.\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "# Group by cluster.\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# Define custom css to format the font and to remove the axis labeling.\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\n",
    "svg.mpld3-figure {\n",
    "margin-left: -200px;}\n",
    "\"\"\"\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(14,6)) #set plot size\n",
    "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "# Iterate through groups to layer the plot.\n",
    "# Note that I use the cluster_name and cluster_color dicts with the 'name' \n",
    "# lookup to return the appropriate color/label.\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, \n",
    "                     label=cluster_names[name], mec='none', \n",
    "                     color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    # Set tooltip using points, labels and the already defined 'css'.\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    # Connect tooltip to fig.\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    # Set tick marks as blank.\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    # Set axis as blank.\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1) # show legend with only one dot\n",
    "\n",
    "# Uncomment  accordingly to show the plot or save it to an html file.\n",
    "mpld3.display() # show the plot\n",
    "# OR\n",
    "#clusterHTML = open(\"clusters.html\", \"w\") # save as an HTML file\n",
    "#clusterHTML.writelines(mpld3.fig_to_html(fig))\n",
    "#clusterHTML.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical document clustering\n",
    "\n",
    "Now try another clustering algorithm. Ward clustering is an agglomerative clustering method, meaning that at each stage, the pair of clusters with minimum between-cluster distance are merged. We used the precomputed cosine distance matrix (dist) to calculate a linkage_matrix, from which we then plot a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "# Define the linkage_matrix using ward clustering pre-computed distances.\n",
    "linkage_matrix = ward(dist) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',         # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "# Uncomment the below to show or save the plot.\n",
    "plt.show()\n",
    "#plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Try changing preprocessing techniques, TF-IDF parameters and observe differences.\n",
    "* Use PCA (see [sci-kit learn docs](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)) instead MDS.\n",
    "* Try a different hierarchical agglomerative clustering. Check [SciPy documentation](https://docs.scipy.org/doc/scipy-0.18.1/reference/cluster.hierarchy.html) for possibilities or provide your own linkage matrix.\n",
    "\n",
    "## References\n",
    "\n",
    "* Notebook is an adaptation by [http://brandonrose.org/clustering](http://brandonrose.org/clustering)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-course-fri] *",
   "language": "python",
   "name": "conda-env-nlp-course-fri-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
