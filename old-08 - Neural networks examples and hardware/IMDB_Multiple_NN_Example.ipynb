{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muliple neural network analysis\n",
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [slavko.zitnik@fri.uni-lj.si](mailto:slavko.zitnik@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "## IMDB sentiment analysis example\n",
    "\n",
    "First we download the IMDB dataset. We present each word with a specific index from a vocabulary of 10000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/slavkoz/anaconda3/envs/nlp-course-fri-2/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/slavkoz/anaconda3/envs/nlp-course-fri-2/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000.\n",
      "First review: \n",
      "\t[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32].\n",
      "First label: 1.\n",
      "Length of first review before padding: 218.\n",
      "\n",
      "After padding:\n",
      "First review: \n",
      "\t[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
      "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
      "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
      "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
      "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
      "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
      "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
      "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
      "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
      "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
      "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
      "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
      "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      " 4472  113  103   32   15   16 5345   19  178   32].\n",
      "Length of first review after padding: 500.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# number of distinct words\n",
    "vocabulary_size = 10000\n",
    "\n",
    "# number of words per review\n",
    "max_review_length = 500\n",
    "\n",
    "# load Keras IMDB movie reviews dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "\n",
    "print(f'Number of reviews: {len(X_train)}.')\n",
    "print(f'First review: \\n\\t{X_train[0]}.')\n",
    "print(f'First label: {y_train[0]}.')\n",
    "print(f'Length of first review before padding: {len(X_train[0])}.')\n",
    "\n",
    "# padding reviews\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print(f\"\\nAfter padding:\")\n",
    "print(f'First review: \\n\\t{X_train[0]}.')\n",
    "print(f'Length of first review after padding: {len(X_train[0])}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping between real words and indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review: \n",
      "\tText: please give this one a miss br br and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite so all you madison fans give this a miss\n",
      "\tLabel: NEGATIVE\n",
      "\n",
      "Last review: \n",
      "\tText: a good ol' boy film is almost required to have car chases a storyline that has a vague resemblance to plot and at least one very pretty country gal with short shorts and a low top the pretty gal is here dressed in designer but the redneck stop there jimmy dean is a natural as a but as a tough guy former sheriff he comes up way short big john is big but he isn't convincing with the bad part of his bug eyed jack is a hoot as always and bo hopkins has been playing this same part for decades ned beatty also does his part in a small role but there is no story it more like an episode of in the heat of the night than a feature film with easily predictable sentiment perhaps the most glaring problem with this movie is charlie daniels singing the theme you know the one it was made famous by jimmy dean\n",
      "\tLabel: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved, so we map the index for our use\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Decode review text\n",
    "def decode_review(text_ids, cls):\n",
    "    text = ' '.join([reverse_word_index.get(i, '?') for i in text_ids if i not in [0,1,2,3]])\n",
    "    label = 'POSITIVE' if cls == 1 else 'NEGATIVE'\n",
    "    return f\"\\tText: {text}\\n\\tLabel: {label}\"\n",
    "\n",
    "# First review\n",
    "print(f\"First review: \\n{decode_review(X_test[0], y_test[0])}\")\n",
    "# Last review\n",
    "print(f\"\\nLast review: \\n{decode_review(X_test[len(X_test)-1], y_test[len(X_test)-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create multiple models and evaluate their performance:\n",
    "\n",
    "* **FFN**: Input to the models are word indices directly fed into a Dense layer.\n",
    "* **FFN with embeddings**: After creation of embedding vectors, the same architecture as in *FFN* is used.\n",
    "* **CNN**: Similar to the *FFN with embeddings* model with a convoluational and pooling layer immediatelly after embedding layer,\n",
    "* **RNN**: Simple RNN model with 100 hidden dimensions and prediction at the end.\n",
    "* **CNN+RNN**: A combination of *CNN* and *RNN* models above.\n",
    "\n",
    "The runtime for the below script should take about a half hour using Tesla V100 32GB GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "196/196 - 1s - loss: 190.9913 - accuracy: 0.5032\n",
      "Epoch 2/20\n",
      "196/196 - 0s - loss: 57.6169 - accuracy: 0.5808\n",
      "Epoch 3/20\n",
      "196/196 - 0s - loss: 23.1519 - accuracy: 0.6620\n",
      "Epoch 4/20\n",
      "196/196 - 0s - loss: 10.7224 - accuracy: 0.7308\n",
      "Epoch 5/20\n",
      "196/196 - 0s - loss: 5.7484 - accuracy: 0.7804\n",
      "Epoch 6/20\n",
      "196/196 - 0s - loss: 3.3715 - accuracy: 0.8194\n",
      "Epoch 7/20\n",
      "196/196 - 0s - loss: 2.1158 - accuracy: 0.8480\n",
      "Epoch 8/20\n",
      "196/196 - 0s - loss: 1.4276 - accuracy: 0.8670\n",
      "Epoch 9/20\n",
      "196/196 - 0s - loss: 1.0375 - accuracy: 0.8863\n",
      "Epoch 10/20\n",
      "196/196 - 0s - loss: 0.7942 - accuracy: 0.9000\n",
      "Epoch 11/20\n",
      "196/196 - 0s - loss: 0.6851 - accuracy: 0.9062\n",
      "Epoch 12/20\n",
      "196/196 - 0s - loss: 0.6019 - accuracy: 0.9124\n",
      "Epoch 13/20\n",
      "196/196 - 0s - loss: 0.4784 - accuracy: 0.9227\n",
      "Epoch 14/20\n",
      "196/196 - 0s - loss: 0.4969 - accuracy: 0.9231\n",
      "Epoch 15/20\n",
      "196/196 - 0s - loss: 0.5318 - accuracy: 0.9202\n",
      "Epoch 16/20\n",
      "196/196 - 0s - loss: 0.6472 - accuracy: 0.9137\n",
      "Epoch 17/20\n",
      "196/196 - 0s - loss: 0.5838 - accuracy: 0.9154\n",
      "Epoch 18/20\n",
      "196/196 - 0s - loss: 0.6152 - accuracy: 0.9145\n",
      "Epoch 19/20\n",
      "196/196 - 0s - loss: 0.6899 - accuracy: 0.9154\n",
      "Epoch 20/20\n",
      "196/196 - 0s - loss: 0.6660 - accuracy: 0.9161\n",
      "Epoch 1/20\n",
      "196/196 - 10s - loss: 0.5698 - accuracy: 0.7166\n",
      "Epoch 2/20\n",
      "196/196 - 9s - loss: 0.1458 - accuracy: 0.9466\n",
      "Epoch 3/20\n",
      "196/196 - 9s - loss: 0.0195 - accuracy: 0.9968\n",
      "Epoch 4/20\n",
      "196/196 - 9s - loss: 0.0019 - accuracy: 0.9999\n",
      "Epoch 5/20\n",
      "196/196 - 9s - loss: 4.3593e-04 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "196/196 - 9s - loss: 2.2028e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "196/196 - 9s - loss: 1.3128e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "196/196 - 9s - loss: 8.4444e-05 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "196/196 - 9s - loss: 5.7175e-05 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "196/196 - 9s - loss: 4.0012e-05 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "196/196 - 9s - loss: 2.8955e-05 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "196/196 - 9s - loss: 2.1555e-05 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "196/196 - 9s - loss: 1.6472e-05 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "196/196 - 9s - loss: 1.2851e-05 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "196/196 - 9s - loss: 1.0156e-05 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "196/196 - 9s - loss: 8.1509e-06 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "196/196 - 9s - loss: 6.6486e-06 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "196/196 - 9s - loss: 5.4695e-06 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "196/196 - 9s - loss: 4.5534e-06 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "196/196 - 9s - loss: 3.8156e-06 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "196/196 - 13s - loss: 0.5590 - accuracy: 0.6516\n",
      "Epoch 2/20\n",
      "196/196 - 12s - loss: 0.2206 - accuracy: 0.9112\n",
      "Epoch 3/20\n",
      "196/196 - 12s - loss: 0.0998 - accuracy: 0.9662\n",
      "Epoch 4/20\n",
      "196/196 - 12s - loss: 0.0308 - accuracy: 0.9910\n",
      "Epoch 5/20\n",
      "196/196 - 12s - loss: 0.0096 - accuracy: 0.9979\n",
      "Epoch 6/20\n",
      "196/196 - 12s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 7/20\n",
      "196/196 - 12s - loss: 2.5037e-04 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "196/196 - 12s - loss: 1.1015e-04 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "196/196 - 12s - loss: 7.5049e-05 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "196/196 - 12s - loss: 5.5278e-05 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "196/196 - 12s - loss: 4.2573e-05 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "196/196 - 12s - loss: 3.3603e-05 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "196/196 - 12s - loss: 2.7135e-05 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "196/196 - 12s - loss: 2.2265e-05 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "196/196 - 12s - loss: 1.8476e-05 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "196/196 - 12s - loss: 1.5476e-05 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "196/196 - 12s - loss: 1.3082e-05 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "196/196 - 12s - loss: 1.1111e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "196/196 - 12s - loss: 9.4945e-06 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "196/196 - 12s - loss: 8.1711e-06 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 0.6691 - accuracy: 0.5777\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 0.5546 - accuracy: 0.7253\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 0.4273 - accuracy: 0.8032\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 41s 103ms/step - loss: 0.6504 - accuracy: 0.5901\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.4049 - accuracy: 0.8193\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.3672 - accuracy: 0.8409\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.3480 - accuracy: 0.8443\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.2893 - accuracy: 0.8845\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.2031 - accuracy: 0.9267\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.1263 - accuracy: 0.9592\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.1129 - accuracy: 0.9615\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.0897 - accuracy: 0.9687\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.0862 - accuracy: 0.9690\n",
      "                  Model Accuracy\n",
      "0                  FFNN   50.84%\n",
      "1  FFNN with Embeddings    87.6%\n",
      "2                   CNN   87.78%\n",
      "3                   RNN   68.76%\n",
      "4               CNN+RNN    80.3%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "import pandas as pd\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# Fully connected neural network\n",
    "model_ffn = Sequential()\n",
    "model_ffn.add(Dense(250, activation='relu',input_dim=max_review_length))\n",
    "model_ffn.add(Dense(1, activation='sigmoid'))\n",
    "model_ffn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_ffn = model_ffn.fit(X_train, y_train, epochs=20, batch_size=128, verbose=2)\n",
    "scores_ffn = model_ffn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Fully connected neural network with embeddings\n",
    "model_ffne = Sequential()\n",
    "model_ffne.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_review_length))\n",
    "model_ffne.add(Flatten())\n",
    "model_ffne.add(Dense(250, activation='relu'))\n",
    "model_ffne.add(Dense(1, activation='sigmoid'))\n",
    "model_ffne.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_ffne = model_ffne.fit(X_train, y_train, epochs=20, batch_size=128, verbose=2)\n",
    "scores_ffne = model_ffne.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Convolutional neural network\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_review_length))\n",
    "model_cnn.add(Conv1D(filters=200, kernel_size=3, padding='same', activation='relu'))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(250, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_cnn = model_cnn.fit(X_train, y_train, epochs=20, batch_size=128, verbose=2)\n",
    "scores_cnn = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Recurrent Neural Network\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_review_length))\n",
    "model_rnn.add(SimpleRNN(100))\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_rnn = model_rnn.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "scores_rnn = model_rnn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Convolutional + Recurrent Neural Network\n",
    "model_cnn_rnn = Sequential()\n",
    "model_cnn_rnn.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_review_length))\n",
    "model_cnn_rnn.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_cnn_rnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_rnn.add(SimpleRNN(100))\n",
    "model_cnn_rnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_cnn_rnn = model_cnn_rnn.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "scores_cnn_rnn = model_cnn_rnn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Evaluation\n",
    "df = pd.DataFrame({'Model': ['FFNN', 'FFNN with Embeddings', 'CNN', 'RNN', 'CNN+RNN'],\n",
    "                   'Accuracy': [str(round(scores_ffn[1]*100, 2)) + '%',\n",
    "                                str(round(scores_ffne[1]*100, 2)) + '%',\n",
    "                                str(round(scores_cnn[1]*100, 2)) + '%',\n",
    "                                str(round(scores_rnn[1]*100, 2)) + '%',\n",
    "                                str(round(scores_cnn_rnn[1]*100, 2)) + '%']})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above should output something similar to the following:\n",
    "\n",
    "```\n",
    "                            Model Accuracy\n",
    "          0                  FFNN   50.79%\n",
    "          1  FFNN with Embeddings   87.64%\n",
    "          2                   CNN   87.55%\n",
    "          3                   RNN   61.14%\n",
    "          4               CNN+RNN   82.62%\n",
    "```\n",
    "\n",
    "Surprisingly, the fully connected neural network with embeddings and convolutional neural network outperform the remaining networks. FFN is a very simple network with a single 250 dimensional dense layer.\n",
    "\n",
    "CNNs can be robust to the position and orientation of learned objects in the image, while the same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. A simple RNN seems not to be very competitive but together with a CNN it achieves a decent performance.\n",
    "\n",
    "Now, let's test the models with some custom examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff92591e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "                                              review FFN FFNE CNN RNN CNN+RNN\n",
      "0                                          I like it   -    +   +   +       +\n",
      "1                                     I dont like it   +    +   +   +       -\n",
      "2  After 30 min I still did not know what the mov...   -    +   -   -       +\n",
      "3  It is so good that I will never ever watch it ...   -    +   -   +       +\n",
      "4                      It is like the Titanic movie!   -    +   +   +       +\n",
      "5           That is the best movie I have ever seen.   -    +   +   +       +\n",
      "6          That is the worst movie I have ever seen.   +    -   -   -       -\n"
     ]
    }
   ],
   "source": [
    "def movie_sentiment(reviews, \n",
    "                    models=[model_ffn, model_ffne, model_cnn, model_rnn, model_cnn_rnn],\n",
    "                    titles=['FFN', 'FFNE', 'CNN', 'RNN', 'CNN+RNN']):\n",
    "    df = pd.DataFrame(columns=['review'] + titles)\n",
    "    i = 0\n",
    "    for review in reviews:\n",
    "        words = set(text_to_word_sequence(review))\n",
    "        words = [word_index[w] for w in words]\n",
    "        words = sequence.pad_sequences([words], maxlen=max_review_length)\n",
    "        df.loc[i] = [review] + titles\n",
    "        df.loc[i]['review'] = review\n",
    "        for j, model in enumerate(models):\n",
    "            proba = model.predict(words)\n",
    "            sentiment = '+' if proba>0.5 else '-'\n",
    "            df.loc[i][titles[j]] = sentiment\n",
    "        i = i + 1\n",
    "    return df\n",
    "    \n",
    "text1 = 'I like it'\n",
    "text2 = 'I dont like it'\n",
    "text3 = 'After 30 min I still did not know what the movie is about'\n",
    "text4 = 'It is so good that I will never ever watch it again. Boring experience.'\n",
    "text5 = \"It is like the Titanic movie!\"\n",
    "text6 = \"That is the best movie I have ever seen.\"\n",
    "text7 = \"That is the worst movie I have ever seen.\"\n",
    "print(movie_sentiment([text1, text2, text3, text4, text5, text6, text7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test above should output something similar to the following:\n",
    "\n",
    "```\n",
    "                                              review FFN FFNE CNN RNN CNN+RNN  | EXPECTED\n",
    "0                                          I like it   -    +   +   +       -  |        + \n",
    "1                                     I dont like it   +    -   +   +       -  |        -\n",
    "2  After 30 min I still did not know what the mov...   +    +   +   -       -  |        -\n",
    "3  It is so good that I will never ever watch it ...   -    +   +   -       -  |        -\n",
    "4                      It is like the Titanic movie!   +    +   +   +       -  |        +\n",
    "5           That is the best movie I have ever seen.   +    +   +   +       -  |        +\n",
    "6          That is the worst movie I have ever seen.   -    -   -   -       -  |        -\n",
    "-----------------------------------------------------------------------------------------\n",
    "                                 CORRECT PREDICTIONS   4    5   4   6       4  |        7\n",
    "```\n",
    "\n",
    "It seems that RNN performed as the best on these few short examples.\n",
    "\n",
    "Try different architectures, models, hyperparameters (e.g. embedding dimensions), ... and you might improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* The example above follows the post by Michel Kana, PhD: [Sentiment analysis: a practical benchmark](https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-course-fri-2]",
   "language": "python",
   "name": "conda-env-nlp-course-fri-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
